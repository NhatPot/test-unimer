# Uni-MuMER: Fine-tuning Th·ªëng nh·∫•t ƒêa nhi·ªám c·ªßa M√¥ h√¨nh Vision-Language cho Nh·∫≠n d·∫°ng Bi·ªÉu th·ª©c To√°n h·ªçc Vi·∫øt tay

<p align="center">
    <a href="https://arxiv.org/abs/2505.23566"><img src="https://img.shields.io/badge/üìÑ-Paper-red"></a>
    <a href="https://huggingface.co/collections/phxember/uni-mumer-68bfba4747e9289232f3d89e"><img src="https://img.shields.io/badge/ü§ó HuggingFace-Data & Models-green"></a>
</p>

## M√¥ t·∫£

Ch√∫ng t√¥i gi·ªõi thi·ªáu Uni-MuMER, m·ªôt ph∆∞∆°ng ph√°p fine-tune ho√†n to√†n m√¥ h√¨nh Qwen2.5-VL-3B cho t√°c v·ª• HMER m√† kh√¥ng thay ƒë·ªïi ki·∫øn tr√∫c c·ªßa n√≥, hi·ªáu qu·∫£ trong vi·ªác t√≠ch h·ª£p ki·∫øn th·ª©c chuy√™n ng√†nh v√†o m·ªôt framework t·ªïng qu√°t. Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i t√≠ch h·ª£p ba t√°c v·ª• d·ª±a tr√™n d·ªØ li·ªáu: Tree-Aware Chain-of-Thought (Tree-CoT) cho l·∫≠p lu·∫≠n kh√¥ng gian c√≥ c·∫•u tr√∫c, Error-Driven Learning (EDL) ƒë·ªÉ gi·∫£m nh·∫ßm l·∫´n gi·ªØa c√°c k√Ω t·ª± tr·ª±c quan t∆∞∆°ng t·ª±, v√† Symbol Counting (SC) ƒë·ªÉ c·∫£i thi·ªán t√≠nh nh·∫•t qu√°n trong nh·∫≠n d·∫°ng c√°c bi·ªÉu th·ª©c d√†i.

![Uni-MuMER](./asserts/fig/main_fig.drawio_00.png)

C√°c th√≠ nghi·ªám tr√™n dataset CROHME v√† HME100K cho th·∫•y Uni-MuMER ƒë·∫°t ƒë∆∞·ª£c hi·ªáu su·∫•t state-of-the-art m·ªõi, v∆∞·ª£t qua m√¥ h√¨nh chuy√™n bi·ªát nh·∫π t·ªët nh·∫•t SSAN 16.31% v√† VLM h√†ng ƒë·∫ßu Gemini2.5-flash 24.42% trong thi·∫øt l·∫≠p zero-shot.

![intro](./asserts/fig/CROHME_00.png)

## üì¢ C·∫≠p nh·∫≠t

- **2025-09-18**: C√¥ng tr√¨nh n√†y ƒë∆∞·ª£c ch·∫•p nh·∫≠n t·∫°i NeurIPS 2025 v·ªõi danh hi·ªáu Spotlight (688/21575).
- **2025-09-09**: Ph√°t h√†nh dataset ([Uni-MuMER-Data](https://huggingface.co/datasets/phxember/Uni-MuMER-Data) v√† [valid/test data](https://drive.google.com/drive/folders/1T8a3WxICZVl1NJ99hu9tuuqqNZoxGhXq?usp=sharing)) v√† m√£ ngu·ªìn training.
- **2025-06-02**: Ph√°t h√†nh tr·ªçng s·ªë m√¥ h√¨nh v√† script inference.

## üîß C√†i ƒë·∫∑t

### Y√™u c·∫ßu h·ªá th·ªëng
- Ubuntu (ho·∫∑c Linux t∆∞∆°ng th√≠ch)
- GPU v·ªõi CUDA (khuy·∫øn ngh·ªã)
- Conda ho·∫∑c Miniconda
- Python 3.10+

### B∆∞·ªõc 1: T·∫°o m√¥i tr∆∞·ªùng conda

```bash
conda create -n unimumer python=3.10 -y
conda activate unimumer
```

### B∆∞·ªõc 2: C√†i ƒë·∫∑t PyTorch v·ªõi CUDA

```bash
# Ki·ªÉm tra phi√™n b·∫£n CUDA
nvidia-smi

# C√†i PyTorch v·ªõi CUDA (thay ƒë·ªïi theo phi√™n b·∫£n CUDA c·ªßa b·∫°n)
# V√≠ d·ª•: CUDA 12.8
conda install pytorch torchvision torchaudio pytorch-cuda=12.8 -c pytorch -c nvidia -y

pip install torchvision

# Ho·∫∑c CUDA 11.8
# conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y
```

### B∆∞·ªõc 3: C√†i ƒë·∫∑t dependencies

```bash
pip install -r requirements.txt
```

### B∆∞·ªõc 4: Gi·∫£i n√©n dataset (n·∫øu c√≥)

```bash
unzip data.zip
```

## üèÉ Inference (D·ª± ƒëo√°n)

**T·∫•t c·∫£ inference ƒë·ªÅu s·ª≠ d·ª•ng 4-bit quantization ƒë·ªÉ ti·∫øt ki·ªám VRAM (~50% so v·ªõi full precision).**

### Merge Checkpoint ƒë·ªÉ Inference

Sau khi train, b·∫°n c·∫ßn merge adapter t·ª´ checkpoint v√†o base model ƒë·ªÉ c√≥ th·ªÉ inference:

```bash
conda activate unimumer

# Merge checkpoint-1
python scripts/merge_checkpoint.py \
    --base-model ./Uni-MuMER-Qwen2.5-VL-3B \
    --checkpoint saves/qwen2.5_vl-3b/qlora/sft/standred/uni-mumer_qlora/checkpoint-1 \
    --output ./Uni-MuMER-Qwen2.5-VL-3B-checkpoint1

# Ho·∫∑c merge checkpoint-2
python scripts/merge_checkpoint.py \
    --base-model ./Uni-MuMER-Qwen2.5-VL-3B \
    --checkpoint saves/qwen2.5_vl-3b/qlora/sft/standred/uni-mumer_qlora/checkpoint-2 \
    --output ./Uni-MuMER-Qwen2.5-VL-3B-checkpoint2
```

**Tham s·ªë:**
- `--base-model`: Th∆∞ m·ª•c ch·ª©a base model (c√≥ `model-00001-of-00002.safetensors`, `model-00002-of-00002.safetensors`)
- `--checkpoint`: Th∆∞ m·ª•c checkpoint ch·ª©a `adapter_model.safetensors`
- `--output`: Th∆∞ m·ª•c l∆∞u model ƒë√£ merge

### Ch·∫°y Inference cho m·ªôt dataset *****

**L∆∞u √Ω:** Script inference s·ª≠ d·ª•ng vLLM ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô inference. ƒê·∫£m b·∫£o ch·∫°y t·ª´ th∆∞ m·ª•c g·ªëc d·ª± √°n.

```bash
conda activate unimumer

# Chuy·ªÉn v·ªÅ th∆∞ m·ª•c g·ªëc d·ª± √°n
cd ~/Uni-MuMER-project

# V√≠ d·ª•: CROHME v·ªõi model ƒë√£ merge
python scripts/vllm_infer.py \
    --input-dir data/CROHME/prompts \
    --output-dir data/CROHME/results \
    --model ./Uni-MuMER-Qwen2.5-VL-3B-checkpoint1 \
    --batch-size 1 \
    --max-tokens 2048

# V√≠ d·ª•: CROHME2023
python scripts/vllm_infer.py \
    --input-dir data/CROHME2023/prompts \
    --output-dir data/CROHME2023/results \
    --model ./Uni-MuMER-Qwen2.5-VL-3B-checkpoint1 \
    --batch-size 1 \
    --max-tokens 2048

# V√≠ d·ª•: HME100K
python scripts/vllm_infer.py \
    --input-dir data/HME100K/prompts \
    --output-dir data/HME100K/results \
    --model ./Uni-MuMER-Qwen2.5-VL-3B-checkpoint1 \
    --batch-size 1 \
    --max-tokens 2048
```

### Ch·∫°y Inference cho t·∫•t c·∫£ datasets

```bash
conda activate unimumer

# Chuy·ªÉn v·ªÅ th∆∞ m·ª•c g·ªëc d·ª± √°n
cd ~/Uni-MuMER-project

# T·∫°o script ƒë∆°n gi·∫£n ƒë·ªÉ ch·∫°y t·∫•t c·∫£
for dataset in CROHME CROHME2023 HME100K Im2LaTeXv2 MathWriting MNE; do
    if [ -d "data/$dataset/prompts" ]; then
        echo "Processing $dataset..."
        python scripts/vllm_infer.py \
            --input-dir data/$dataset/prompts \
            --output-dir data/$dataset/results \
            --model ./Uni-MuMER-Qwen2.5-VL-3B-checkpoint1 \
            --batch-size 1 \
            --max-tokens 2048
    fi
done
```

### Tham s·ªë Inference

- `--model`: ƒê∆∞·ªùng d·∫´n ƒë·∫øn model ƒë√£ merge (b·∫Øt bu·ªôc, v√≠ d·ª•: `./Uni-MuMER-Qwen2.5-VL-3B-checkpoint1`)
- `--input-dir`: Th∆∞ m·ª•c ch·ª©a file JSON prompts (b·∫Øt bu·ªôc, v√≠ d·ª•: `data/CROHME/prompts`)
- `--output-dir`: Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£ (b·∫Øt bu·ªôc, v√≠ d·ª•: `data/CROHME/results`)
- `--batch-size`: Batch size cho vLLM (m·∫∑c ƒë·ªãnh: 32768, c√≥ th·ªÉ gi·∫£m xu·ªëng 1 n·∫øu thi·∫øu VRAM)
- `--max-tokens`: S·ªë token t·ªëi ƒëa ƒë·ªÉ generate (m·∫∑c ƒë·ªãnh: 2048)
- `--temperature`: Temperature cho sampling (m·∫∑c ƒë·ªãnh: 0.2)
- `--top-p`: Top-p sampling (m·∫∑c ƒë·ªãnh: 0.8)
- `--suffix`: Suffix cho file output (m·∫∑c ƒë·ªãnh: `_pred`)

### Xem k·∫øt qu·∫£

```bash
# Xem k·∫øt qu·∫£ CROHME
cat data/CROHME/results/crohme_2014_results.txt
cat data/CROHME/results/crohme_2016_results.txt
cat data/CROHME/results/crohme_2019_results.txt

# Xem k·∫øt qu·∫£ HME100K
cat data/HME100K/results/hme100k_test_results.txt
```

## üèãÔ∏è Training

**Training s·ª≠ d·ª•ng QLoRA + 4-bit quantization ƒë·ªÉ ti·∫øt ki·ªám VRAM (~50-70% so v·ªõi full precision).**

### C√†i ƒë·∫∑t LLaMA-Factory

Training ph·ª• thu·ªôc v√†o [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory):

```bash
conda activate unimumer

# Clone LLaMA-Factory (n·∫øu ch∆∞a c√≥)
if [ ! -d "train/LLaMA-Factory" ]; then
    git clone https://github.com/hiyouga/LLaMA-Factory.git train/LLaMA-Factory
fi

# C√†i ƒë·∫∑t dependencies cho training
cd train/LLaMA-Factory
pip install -e .
cd ../..
```

### Training Data

**Training data ƒë∆∞·ª£c t·ª± ƒë·ªông t·∫£i t·ª´ HuggingFace khi training.**

C·∫•u h√¨nh ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p ƒë·ªÉ t·ª± ƒë·ªông download datasets t·ª´ `phxember/Uni-MuMER-Data` khi ch·∫°y training. L·∫ßn ƒë·∫ßu ch·∫°y s·∫Ω m·∫•t v√†i ph√∫t ƒë·ªÉ download, sau ƒë√≥ s·∫Ω ƒë∆∞·ª£c cache.

**L∆∞u √Ω:**
- C·∫ßn k·∫øt n·ªëi internet ƒë·ªÉ download datasets
- Datasets s·∫Ω ƒë∆∞·ª£c cache trong `~/.cache/huggingface/`
- N·∫øu c·∫ßn, c√≥ th·ªÉ login HuggingFace: `huggingface-cli login`

### Ch·∫°y Training

```bash
conda activate unimumer

# Ch·∫°y training v·ªõi QLoRA + 4-bit quantization
# Datasets s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông download t·ª´ HuggingFace
llamafactory-cli train train/Uni-MuMER-train.yaml
```

### C·∫•u h√¨nh Training

File `train/Uni-MuMER-train.yaml` ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh v·ªõi:
- **QLoRA + 4-bit quantization**: Gi·∫£m VRAM usage ƒë√°ng k·ªÉ
- **quantization_bit: 4**: S·ª≠ d·ª•ng 4-bit quantization
- **quantization_type: nf4**: NormalFloat4 quantization
- **lora_rank: 64**: LoRA rank
- **lora_alpha: 16**: LoRA alpha
- **per_device_train_batch_size: 2**: Batch size ph√π h·ª£p v·ªõi quantization

**∆Øu ƒëi·ªÉm c·ªßa QLoRA:**
- Gi·∫£m VRAM usage ~50-70% so v·ªõi full precision training
- C√≥ th·ªÉ train tr√™n GPU nh·ªè h∆°n (v√≠ d·ª•: RTX 3090, A6000)
- V·∫´n gi·ªØ ƒë∆∞·ª£c hi·ªáu su·∫•t t·ªët
- Ch·ªâ train LoRA adapters, kh√¥ng c·∫ßn l∆∞u to√†n b·ªô model weights

## ‚öôÔ∏è 4-bit Quantization

T·∫•t c·∫£ inference v√† training ƒë·ªÅu s·ª≠ d·ª•ng 4-bit quantization v·ªõi BitsAndBytes ƒë·ªÉ ti·∫øt ki·ªám VRAM:

### C·∫•u h√¨nh Quantization

```python
BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,  # Double quantization ƒë·ªÉ gi·∫£m th√™m memory
    bnb_4bit_quant_type="nf4",        # NormalFloat4 quantization
    bnb_4bit_compute_dtype=torch.bfloat16  # Compute dtype
)
```

### L·ª£i √≠ch

- **Model size**: Gi·∫£m t·ª´ ~3GB xu·ªëng ~1.5GB
- **VRAM usage (Inference)**: Gi·∫£m t·ª´ ~6GB xu·ªëng ~2-3GB
- **VRAM usage (Training)**: Gi·∫£m t·ª´ ~60-80GB xu·ªëng ~20-30GB
- **ƒê·ªô ch√≠nh x√°c**: V·∫´n gi·ªØ ƒë∆∞·ª£c ƒë·ªô ch√≠nh x√°c t·ªët

## ‚ö†Ô∏è Troubleshooting

### L·ªói "bitsandbytes not found"

```bash
pip install bitsandbytes>=0.41.0
```

### L·ªói OOM (Out of Memory)

1. **Gi·∫£m batch size**:
   ```bash
   --batch-size 1
   ```

2. **Gi·∫£m max tokens**:
   ```bash
   --max-tokens 1024
   ```

3. **Clear GPU cache**:
   ```bash
   python -c "import torch; torch.cuda.empty_cache()"
   ```

4. **Ki·ªÉm tra processes kh√°c**:
   ```bash
   nvidia-smi
   # Kill processes kh√¥ng c·∫ßn thi·∫øt
```

### L·ªói CUDA kh√¥ng kh·ªõp

Ki·ªÉm tra v√† c√†i l·∫°i PyTorch v·ªõi ƒë√∫ng phi√™n b·∫£n CUDA:
```bash
nvidia-smi  # Ki·ªÉm tra phi√™n b·∫£n CUDA
conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia -y
```

### L·ªói thi·∫øu module

```bash
conda activate unimumer
pip install -r requirements.txt
```

## üìù Tr√≠ch d·∫´n

N·∫øu b·∫°n th·∫•y Uni-MuMER h·ªØu √≠ch cho nghi√™n c·ª©u c·ªßa m√¨nh, vui l√≤ng tr√≠ch d·∫´n b√†i b√°o c·ªßa ch√∫ng t√¥i:

```bibtex
@article{li2025unimumer,
  title = {Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for Handwritten Mathematical Expression Recognition},
  author = {Li, Yu and Jiang, Jin and Zhu, Jianhua and Peng, Shuai and Wei, Baole and Zhou, Yuxuan and Gao, Liangcai},
  year = {2025},
  journal={arXiv preprint arXiv:2505.23566},
}
```

## üôè L·ªùi c·∫£m ∆°n

C·∫£m ∆°n c√°c d·ª± √°n sau:

- [CoMER](https://github.com/Green-Wood/CoMER)
- [PosFormer](https://github.com/SJTU-DeepVisionLab/PosFormer)
- [TAMER](https://github.com/qingzhenduyu/TAMER)
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)
- [MathNet](https://github.com/felix-schmitt/MathNet)
